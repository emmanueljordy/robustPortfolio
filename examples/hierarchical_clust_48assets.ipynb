{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dcor as dc\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import norm\n",
    "from math import sqrt,  tanh, ceil, log, cos, pi, sin\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "original_dir = os.getcwd()\n",
    "os.chdir('..\\\\src\\\\robustOptimPack\\\\wrapping')\n",
    "from wrapping_funcs import *\n",
    "os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 50\n",
    "n_obs_vec = [100,250,500]\n",
    "res_mat_noout= np.ones((len(n_obs_vec),n_reps, 4 ))\n",
    "delta_t = 1/360\n",
    "mean_vec = np.array([-0.45, -0.25, -0.2, -0.15, -0.1, -0.05, 0.05, 0.08, 0.1, 0.15, 0.2, 0.21, 0.25, 0.3, 0.45])*delta_t\n",
    "sd_vec = np.array([0.5, 0.2, 0.3, 0.25, 0.3, 0.2, 0.25, 0.23, 0.2, 0.4, 0.2, 0.25, 0.3, 0.15, 0.5]) *np.sqrt(delta_t)\n",
    "sd_white_noise = sqrt(0.01)\n",
    "n_clusts = 24\n",
    "n_assets = 48\n",
    "link_method = 'average'\n",
    "out_fraction = 0.4\n",
    "assets_in_groups = np.repeat([6,5,4,3,2,1], [1,1,2,2,5,13])\n",
    "out_dist = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic simulation without outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mat_noout= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         data_assets = np.zeros((n_obs, 1))\n",
    "         \n",
    "         for group_ind in assets_in_groups: \n",
    "             #print('group_ind is ', group_ind)\n",
    "             #initialise data matrix for that group: \n",
    "             data_group_ind = np.zeros((n_obs, group_ind))\n",
    "             #generate normal variate for that group  \n",
    "             y_l_t = np.random.normal(size = n_obs) \n",
    "             \n",
    "             for asset_ind in range(group_ind): \n",
    "                 \n",
    "                 #sample mean and volatility for current asset in current group \n",
    "                 asset_i_mean = np.random.choice(mean_vec, size = 1)\n",
    "                 asset_i_vol = np.random.choice(sd_vec, size = 1)\n",
    "                 \n",
    "                 data_group_ind[:,asset_ind] = asset_i_mean + np.multiply(y_l_t, asset_i_vol)\n",
    "                 #print('data_group shape ', data_group_ind.shape)\n",
    "             data_assets = np.column_stack((data_assets, data_group_ind))\n",
    "             #print('data_asset shape ', data_assets.shape)\n",
    "         \n",
    "         dat_sim = data_assets[:,1:]\n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                          1,1,1,1,1,1,1,1,1,1]).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.mean(res_mat_noout[0], axis = 0))\n",
    "print( \"n = 100: \", np.mean(res_mat_noout[1], axis = 0))\n",
    "print( \"n = 250: \", np.mean(res_mat_noout[2], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation with comonotonic setting and rowwise outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 48\n",
    "res_mat_out_rowwise= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         data_assets = np.zeros((n_obs, 1))\n",
    "         \n",
    "         for group_ind in assets_in_groups: \n",
    "             #print('group_ind is ', group_ind)\n",
    "             #initialise data matrix for that group: \n",
    "             data_group_ind = np.zeros((n_obs, group_ind))\n",
    "             #generate normal variate for that group  \n",
    "             y_l_t = np.random.normal(size = n_obs) \n",
    "             \n",
    "             for asset_ind in range(group_ind): \n",
    "                 \n",
    "                 #sample mean and volatility for current asset in current group \n",
    "                 asset_i_mean = np.random.choice(mean_vec, size = 1)\n",
    "                 asset_i_vol = np.random.choice(sd_vec, size = 1)\n",
    "                 \n",
    "                 data_group_ind[:,asset_ind] = asset_i_mean + np.multiply(y_l_t, asset_i_vol)\n",
    "                 #print('data_group shape ', data_group_ind.shape)\n",
    "             data_assets = np.column_stack((data_assets, data_group_ind))\n",
    "             #print('data_asset shape ', data_assets.shape)\n",
    "         \n",
    "         dat_sim = data_assets[:,1:]\n",
    "         \n",
    "         # generate casewise outliers \n",
    "         outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         dat_sim[outliers_ind,:] = (np.max(dat_sim) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_rowwise[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_rowwise[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_rowwise[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_rowwise[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                          1,1,1,1,1,1,1,1,1,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.mean(res_mat_out_rowwise[0], axis = 0))\n",
    "print( \"n = 100: \", np.mean(res_mat_out_rowwise[1], axis = 0))\n",
    "print( \"n = 250: \", np.mean(res_mat_out_rowwise[2], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cellwise outliers on 16 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 16\n",
    "res_mat_out_16= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         data_assets = np.zeros((n_obs, 1))\n",
    "         \n",
    "         # generate index of outlying columns \n",
    "         out_cols = np.sort( np.random.randint(n_assets, size = out_dim))\n",
    "         out_col_counter = 0\n",
    "         \n",
    "         \n",
    "         for group_ind in assets_in_groups: \n",
    "             #print('group_ind is ', group_ind)\n",
    "             #initialise data matrix for that group: \n",
    "             data_group_ind = np.zeros((n_obs, group_ind))\n",
    "             #generate normal variate for that group  \n",
    "             y_l_t = np.random.normal(size = n_obs) \n",
    "             \n",
    "             for asset_ind in range(group_ind): \n",
    "                 \n",
    "                 #sample mean and volatility for current asset in current group \n",
    "                 asset_i_mean = np.random.choice(mean_vec, size = 1)\n",
    "                 asset_i_vol = np.random.choice(sd_vec, size = 1)\n",
    "                 \n",
    "                 data_group_ind[:,asset_ind] = asset_i_mean + np.multiply(y_l_t, asset_i_vol)\n",
    "                 #print('data_group shape ', data_group_ind.shape)\n",
    "                 \n",
    "                 # generate outliers if the current column should contain outliers \n",
    "                 if out_col_counter in out_cols: \n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                     outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "                     data_group_ind[outliers_ind,asset_ind] = (np.max(data_group_ind[:,asset_ind]) *out_dist)/sqrt(out_dim)\n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                 out_col_counter +=1 \n",
    "             data_assets = np.column_stack((data_assets, data_group_ind))\n",
    "             #print('data_asset shape ', data_assets.shape)\n",
    "         \n",
    "         dat_sim = data_assets[:,1:]\n",
    "         \n",
    "         # generate casewise outliers \n",
    "         outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         dat_sim[outliers_ind,:] = (np.max(dat_sim) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_16[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_16[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_16[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_16[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                          1,1,1,1,1,1,1,1,1,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 100: \", np.mean(res_mat_out_16[0], axis = 0))\n",
    "print( \"n = 250: \", np.mean(res_mat_out_16[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_out_16[2], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cellwise outliers on 32 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 32\n",
    "res_mat_out_32= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         data_assets = np.zeros((n_obs, 1))\n",
    "         \n",
    "         # generate index of outlying columns \n",
    "         out_cols = np.sort( np.random.randint(n_assets, size = out_dim))\n",
    "         out_col_counter = 0\n",
    "         \n",
    "         \n",
    "         for group_ind in assets_in_groups: \n",
    "             #print('group_ind is ', group_ind)\n",
    "             #initialise data matrix for that group: \n",
    "             data_group_ind = np.zeros((n_obs, group_ind))\n",
    "             #generate normal variate for that group  \n",
    "             y_l_t = np.random.normal(size = n_obs) \n",
    "             \n",
    "             for asset_ind in range(group_ind): \n",
    "                 \n",
    "                 #sample mean and volatility for current asset in current group \n",
    "                 asset_i_mean = np.random.choice(mean_vec, size = 1)\n",
    "                 asset_i_vol = np.random.choice(sd_vec, size = 1)\n",
    "                 \n",
    "                 data_group_ind[:,asset_ind] = asset_i_mean + np.multiply(y_l_t, asset_i_vol)\n",
    "                 #print('data_group shape ', data_group_ind.shape)\n",
    "                 \n",
    "                 # generate outliers if the current column should contain outliers \n",
    "                 if out_col_counter in out_cols: \n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                     outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "                     data_group_ind[outliers_ind,asset_ind] = (np.max(data_group_ind[:,asset_ind]) *out_dist)/sqrt(out_dim)\n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                 out_col_counter +=1 \n",
    "             data_assets = np.column_stack((data_assets, data_group_ind))\n",
    "             #print('data_asset shape ', data_assets.shape)\n",
    "         \n",
    "         dat_sim = data_assets[:,1:]\n",
    "         \n",
    "         # generate casewise outliers \n",
    "         outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         dat_sim[outliers_ind,:] = (np.max(dat_sim) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_32[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_32[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_32[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_32[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                          1,1,1,1,1,1,1,1,1,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 100: \", np.mean(res_mat_out_32[0], axis = 0))\n",
    "print( \"n = 250: \", np.mean(res_mat_out_32[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_out_32[2], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cellwise outliers on 48 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 48\n",
    "res_mat_out_48= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         data_assets = np.zeros((n_obs, 1))\n",
    "         \n",
    "         # generate index of outlying columns \n",
    "         out_cols = np.sort( np.random.randint(n_assets, size = out_dim))\n",
    "         out_col_counter = 0\n",
    "         \n",
    "         \n",
    "         for group_ind in assets_in_groups: \n",
    "             #print('group_ind is ', group_ind)\n",
    "             #initialise data matrix for that group: \n",
    "             data_group_ind = np.zeros((n_obs, group_ind))\n",
    "             #generate normal variate for that group  \n",
    "             y_l_t = np.random.normal(size = n_obs) \n",
    "             \n",
    "             for asset_ind in range(group_ind): \n",
    "                 \n",
    "                 #sample mean and volatility for current asset in current group \n",
    "                 asset_i_mean = np.random.choice(mean_vec, size = 1)\n",
    "                 asset_i_vol = np.random.choice(sd_vec, size = 1)\n",
    "                 \n",
    "                 data_group_ind[:,asset_ind] = asset_i_mean + np.multiply(y_l_t, asset_i_vol)\n",
    "                 #print('data_group shape ', data_group_ind.shape)\n",
    "                 \n",
    "                 # generate outliers if the current column should contain outliers \n",
    "                 if out_col_counter in out_cols: \n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                     outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "                     data_group_ind[outliers_ind,asset_ind] = (np.max(data_group_ind[:,asset_ind]) *out_dist)/sqrt(out_dim)\n",
    "                     #print('out col ', out_col_counter, ' max ', np.max(data_group_ind[:,asset_ind]))\n",
    "                 out_col_counter +=1 \n",
    "             data_assets = np.column_stack((data_assets, data_group_ind))\n",
    "             #print('data_asset shape ', data_assets.shape)\n",
    "         \n",
    "         dat_sim = data_assets[:,1:]\n",
    "         \n",
    "         # generate casewise outliers \n",
    "         outliers_ind = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         dat_sim[outliers_ind,:] = (np.max(dat_sim) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_48[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_48[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "        \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_48[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                        1,1,1,1,1,1,1,1,1,1]).astype(int))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_48[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat(np.linspace(start=0, stop=23, num=24), [6,5,4,4,3,3,2,2,2,2,2,1,1,1,\n",
    "                                                                                          1,1,1,1,1,1,1,1,1,1]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 100: \", np.mean(res_mat_out_48[0], axis = 0))\n",
    "print( \"n = 250: \", np.mean(res_mat_out_48[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_out_48[2], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5311ec2bf6f7d85cab2d580bd724fa76bee655ef41b8e1c97075e57971827f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
