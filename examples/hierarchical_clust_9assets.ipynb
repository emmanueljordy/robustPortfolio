{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dcor as dc\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import norm\n",
    "from math import sqrt,  tanh, ceil, log, cos, pi, sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "original_dir = os.getcwd()\n",
    "os.chdir('..\\\\src\\\\robustOptimPack\\\\wrapping')\n",
    "from wrapping_funcs import *\n",
    "os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 100\n",
    "n_obs_vec = [50,100,500,1000]\n",
    "res_mat_noout= np.ones((len(n_obs_vec),n_reps, 4 ))\n",
    "mean_vec = [0, 0,0]\n",
    "cov_mat = [[1, 0,0], [0, 1,0], [0,0,1]]  # diagonal covariance\n",
    "sd_white_noise = sqrt(0.01)\n",
    "n_clusts = 3\n",
    "link_method = 'average'\n",
    "out_fraction = 0.2\n",
    "out_dist = 6\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic simulation with no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         \n",
    "         sd_white_noise = sqrt(0.01)\n",
    "        \n",
    "         \n",
    "         x_1 = np.random.uniform(0,1,n_obs)\n",
    "         x_1 = -1 + x_1*2\n",
    "        \n",
    "         u =   np.random.uniform(0,1,n_obs)\n",
    "         v =   np.random.uniform(0,1,n_obs) \n",
    "         x_4 = np.sqrt(-2 *np.log(u) ) *np.cos(2*pi*v)\n",
    "         x_7 = np.sqrt(-2 *np.log(u) ) *np.sin(2*pi*v)\n",
    "         \n",
    "         x_2 = np.tanh(x_1) + np.power(x_1,2)\n",
    "         x_3 = 2*np.sin(np.abs(x_1)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         x_5 = np.sin(x_4) + np.tanh(x_4) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         x_6 = np.power(x_4,2) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         x_8 = np.abs(x_7) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         x_9 = np.sin(np.abs(x_7)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         dat_sim = np.vstack((x_1,x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9)).T\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_noout[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         \n",
    "\n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "        \n",
    "        \n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.mean(res_mat_noout[0], axis = 0))\n",
    "print( \"n = 100: \", np.mean(res_mat_noout[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_noout[2], axis = 0))\n",
    "print( \"n = 1000: \", np.mean(res_mat_noout[3], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(dat_sim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulations with outliers in 3 variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 3\n",
    "res_mat_out_dim3_dist6= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     \n",
    "     for cur_rep in range(n_reps):\n",
    "        \n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         \n",
    "         sd_white_noise = sqrt(0.01)\n",
    "        \n",
    "         \n",
    "         x_1 = np.random.uniform(0,1,n_obs)\n",
    "         x_1 = -1 + x_1*2\n",
    "        \n",
    "         u =   np.random.uniform(0,1,n_obs)\n",
    "         v =   np.random.uniform(0,1,n_obs) \n",
    "         x_4 = np.sqrt(-2 *np.log(u) ) *np.cos(2*pi*v)\n",
    "         x_7 = np.sqrt(-2 *np.log(u) ) *np.sin(2*pi*v)\n",
    "         \n",
    "         x_2 = np.tanh(x_1) + np.power(x_1,2)\n",
    "         # add outliers to x_2\n",
    "         out_ind_x_2 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_2[   out_ind_x_2] = (np.max(x_2) *out_dist)/sqrt(out_dim)\n",
    "         x_3 = 2*np.sin(np.abs(x_1)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         # add outliers to x_5\n",
    "         x_5 = np.sin(x_4) + np.tanh(x_4) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_5 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_5[   out_ind_x_5] = (np.max(x_5) *out_dist)/sqrt(out_dim)\n",
    "         x_6 = np.power(x_4,2) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         # add outliers to x_8\n",
    "         x_8 = np.abs(x_7) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_8 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_8[out_ind_x_8] = (np.max(x_8) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         x_9 = np.sin(np.abs(x_7)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         \n",
    "         dat_sim = np.vstack((x_1,x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9)).T\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim3_dist6[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim3_dist6[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim3_dist6[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim3_dist6[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.mean(res_mat_out_dim3_dist6[0], axis = 0))\n",
    "print( \"n = 100: \", np.mean(res_mat_out_dim3_dist6[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_out_dim3_dist6[2], axis = 0))\n",
    "print( \"n = 1000: \", np.mean(res_mat_out_dim3_dist6[3], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation with outliers in 6 variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 6\n",
    "res_mat_out_dim6_dist6= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     for cur_rep in range(n_reps):\n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         \n",
    "         sd_white_noise = sqrt(0.01)\n",
    "        \n",
    "         \n",
    "         x_1 = np.random.uniform(0,1,n_obs)\n",
    "         x_1 = -1 + x_1*2\n",
    "        \n",
    "         u =   np.random.uniform(0,1,n_obs)\n",
    "         v =   np.random.uniform(0,1,n_obs) \n",
    "         x_4 = np.sqrt(-2 *np.log(u) ) *np.cos(2*pi*v)\n",
    "         x_7 = np.sqrt(-2 *np.log(u) ) *np.sin(2*pi*v)\n",
    "         \n",
    "         x_2 = np.tanh(x_1) + np.power(x_1,2)\n",
    "         # add outliers to x_2\n",
    "         out_ind_x_2 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_2[   out_ind_x_2] = (np.max(x_2) *out_dist)/sqrt(out_dim)\n",
    "          # add outliers to x_3\n",
    "         x_3 = 2*np.sin(np.abs(x_1)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_3 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_3[out_ind_x_3] = (np.max(x_3) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         # add outliers to x_5\n",
    "         x_5 = np.sin(x_4) + np.tanh(x_4) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_5 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_5[   out_ind_x_5] = (np.max(x_5) *out_dist)/sqrt(out_dim)\n",
    "         # add outliers to x_6\n",
    "         x_6 = np.power(x_4,2) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_6 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_6[out_ind_x_6] = (np.max(x_6) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         # add outliers to x_8\n",
    "         x_8 = np.abs(x_7) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_8 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_8[out_ind_x_8] = (np.max(x_8) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "          # add outliers to x_9\n",
    "         x_9 = np.sin(np.abs(x_7)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_9 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_9[out_ind_x_9] = (np.max(x_9) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         dat_sim = np.vstack((x_1,x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9)).T\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim6_dist6[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim6_dist6[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim6_dist6[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim6_dist6[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.mean(res_mat_out_dim6_dist6[0], axis = 0))\n",
    "print( \"n = 100: \", np.mean(res_mat_out_dim6_dist6[1], axis = 0))\n",
    "print( \"n = 500: \", np.mean(res_mat_out_dim6_dist6[2], axis = 0))\n",
    "print( \"n = 1000: \", np.mean(res_mat_out_dim6_dist6[3], axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulations with outliers in the 9 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 9\n",
    "res_mat_out_dim9_dist6= np.ones((len(n_obs_vec),n_reps, 4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for cur_nobs in range(len(n_obs_vec)): \n",
    "     n_obs = n_obs_vec[cur_nobs]\n",
    "     #print(\"current n_obs is \", n_obs)\n",
    "     for cur_rep in range(n_reps):\n",
    "         np.random.seed(cur_rep)\n",
    "         #print('current rep', cur_rep+1)\n",
    "         \n",
    "         sd_white_noise = sqrt(0.01)\n",
    "        \n",
    "         \n",
    "         x_1 = np.random.uniform(0,1,n_obs)\n",
    "         x_1 = -1 + x_1*2\n",
    "        \n",
    "         u =   np.random.uniform(0,1,n_obs)\n",
    "         v =   np.random.uniform(0,1,n_obs) \n",
    "         x_4 = np.sqrt(-2 *np.log(u) ) *np.cos(2*pi*v)\n",
    "         x_7 = np.sqrt(-2 *np.log(u) ) *np.sin(2*pi*v)\n",
    "         \n",
    "         x_2 = np.tanh(x_1) + np.power(x_1,2)\n",
    "         # add outliers to x_2\n",
    "         out_ind_x_2 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_2[   out_ind_x_2] = (np.max(x_2) *out_dist)/sqrt(out_dim)\n",
    "          # add outliers to x_3\n",
    "         x_3 = 2*np.sin(np.abs(x_1)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_3 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_3[out_ind_x_3] = (np.max(x_3) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         # add outliers to x_5\n",
    "         x_5 = np.sin(x_4) + np.tanh(x_4) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_5 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_5[   out_ind_x_5] = (np.max(x_5) *out_dist)/sqrt(out_dim)\n",
    "         # add outliers to x_6\n",
    "         x_6 = np.power(x_4,2) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_6 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_6[out_ind_x_6] = (np.max(x_6) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         # add outliers to x_8\n",
    "         x_8 = np.abs(x_7) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_8 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_8[out_ind_x_8] = (np.max(x_8) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "          # add outliers to x_9\n",
    "         x_9 = np.sin(np.abs(x_7)) + np.random.normal(scale = sd_white_noise,size = n_obs)\n",
    "         out_ind_x_9 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_9[out_ind_x_9] = (np.max(x_9) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         # add outliers to x_1\n",
    "         out_ind_x_1 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_1[out_ind_x_1] = (np.max(x_1) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         \n",
    "         # add outliers to x_4\n",
    "         out_ind_x_4 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_4[out_ind_x_4] = (np.max(x_4) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         #add outliers to x_7\n",
    "         out_ind_x_7 = np.random.randint(n_obs, size= round(out_fraction *n_obs) )\n",
    "         x_7[out_ind_x_7] = (np.max(x_7) *out_dist)/sqrt(out_dim)\n",
    "         \n",
    "         \n",
    "         dat_sim = np.vstack((x_1,x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9)).T\n",
    "         \n",
    "         #correlation based clustering\n",
    "         dat_sim_cor = np.corrcoef(dat_sim.T)\n",
    "         dat_sim_cor_diss = squareform(np.round((1- dat_sim_cor)/2,6))\n",
    "         cor_clust = linkage(dat_sim_cor_diss, method =link_method )\n",
    "         cor_clusts = cut_tree(cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim9_dist6[cur_nobs][cur_rep][0] = np.mean(cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #dcor based clustering\n",
    "         dat_sim_dcor = pairwise_dcor(dat_sim)\n",
    "         dat_sim_dcor_diss = squareform(np.round((1- dat_sim_dcor)/2,6))\n",
    "         dcor_clust = linkage(dat_sim_dcor_diss, method = link_method)\n",
    "         dcor_clusts = cut_tree(dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim9_dist6[cur_nobs][cur_rep][1] = np.mean(dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "         \n",
    "         #w_cor based clustering \n",
    "         dat_sim_w_cor = wrapped_covariance_correlation(dat_sim)[1]\n",
    "         dat_sim_w_cor_diss = squareform(np.round((1- dat_sim_w_cor)/2,6))\n",
    "         w_cor_clust = linkage(dat_sim_w_cor_diss, method =link_method )\n",
    "         w_cor_clusts = cut_tree(w_cor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim9_dist6[cur_nobs][cur_rep][2] = np.mean(w_cor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n",
    "\n",
    "         #w_dcor based clustering \n",
    "         dat_sim_w_dcor = wrapped_dcor(dat_sim)\n",
    "         dat_sim_w_dcor_diss = squareform(np.round((1- dat_sim_w_dcor)/2,6))\n",
    "         w_dcor_clust = linkage(dat_sim_w_dcor_diss, method =link_method )\n",
    "         w_dcor_clusts = cut_tree(w_dcor_clust, n_clusters= n_clusts)\n",
    "         res_mat_out_dim9_dist6[cur_nobs][cur_rep][3] = np.mean(w_dcor_clusts.flatten() == np.repeat([0,1,2],[3,3,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"n = 50: \", np.round(np.mean(res_mat_out_dim9_dist6[0], axis = 0),2))\n",
    "print( \"n = 100: \", np.round(np.mean(res_mat_out_dim9_dist6[1], axis = 0),2))\n",
    "print( \"n = 500: \", np.round(np.mean(res_mat_out_dim9_dist6[2], axis = 0),2))\n",
    "print( \"n = 1000: \", np.round(np.mean(res_mat_out_dim9_dist6[3], axis = 0),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5311ec2bf6f7d85cab2d580bd724fa76bee655ef41b8e1c97075e57971827f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
